{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "import itertools as it\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.nonparametric import nelson_aalen_estimator\n",
    "\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import Normalize, to_hex\n",
    "from matplotlib.cm import get_cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_gen(x, beta, l, k, max_time=1000, death_prob=0.1):\n",
    "    \n",
    "    v = np.random.rand()\n",
    "    time = np.power(-np.log(v) / (l * np.exp(np.dot(x, beta))), 1 / k)\n",
    "    time = max_time if time > max_time else np.round(time)\n",
    "    event = bool(np.random.choice([0, 1], 1, p=[death_prob, 1 - death_prob]))\n",
    "    \n",
    "    return (event, time)\n",
    "\n",
    "\n",
    "def data_gen(n_points, n_features, l, k, max_time):\n",
    "    \n",
    "    beta_true = np.round(np.random.uniform(-1, 1, n_features), 2)\n",
    "    X = np.random.uniform(0, 1, size=(n_points, n_features))\n",
    "    y = np.array([time_gen(x, beta_true, l, k, max_time) for x in X], dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "    \n",
    "    return beta_true, X, y\n",
    "\n",
    "\n",
    "def uniform_within_ball(origin, radius, n_points):\n",
    "    \n",
    "    dim = origin.size\n",
    "    \n",
    "    gauss_points = np.random.normal(loc=0, scale=1, size=(n_points, dim))\n",
    "    norms = np.linalg.norm(gauss_points, axis=1)\n",
    "    unf_points = np.power(np.random.uniform(low=0, high=1, size=n_points), 1 / dim)\n",
    "\n",
    "    points = origin + radius * np.array([gp / n * up for gp, n, up in zip(gauss_points, norms, unf_points)])\n",
    "    \n",
    "    return points\n",
    "\n",
    "\n",
    "def dichotomy(func, x_min=-1000, x_max=1000, tol=1e-9):\n",
    "    \n",
    "    while x_max - x_min > tol:\n",
    "        point = 0.5 * (x_min + x_max)\n",
    "        f_val = func(point)\n",
    "        \n",
    "        if f_val * func(x_max) < 0:\n",
    "            x_min = point\n",
    "        else:\n",
    "            x_max = point\n",
    "            \n",
    "    point = 0.5 * (x_min + x_max)\n",
    "\n",
    "    return point\n",
    "\n",
    "\n",
    "def expand(args, values, new_args, zero_val, increasing=True):\n",
    "    \n",
    "    indexes = np.searchsorted(args, new_args, side='right') - 1\n",
    "    new_values = np.where(indexes == -1, zero_val, values[indexes])\n",
    "    \n",
    "    if increasing:\n",
    "        new_values = np.where(new_values < zero_val, zero_val, new_values)\n",
    "        \n",
    "    return new_values\n",
    "\n",
    "\n",
    "def time_unification(pred_times, pred_data, pred_type, y_train, zero_val):\n",
    "        \n",
    "    if not pred_type in ['sf', 'chf']:\n",
    "        raise ValueError('Models can produce either a survival function (sf) or a cumulative hazard function (chf)')\n",
    "    \n",
    "    times = np.unique([t for _, t in y_train])\n",
    "    times = np.array([0] + times.tolist())\n",
    "    \n",
    "    pred_data = pred_data.reshape(1, -1) if len(pred_data.shape) == 1 else pred_data\n",
    "    \n",
    "    if pred_type == 'sf':\n",
    "        pred_data = np.array([expand(pred_times, pred, times, zero_val, increasing=False) for pred in pred_data])\n",
    "    else:\n",
    "        pred_data = np.array([expand(pred_times, pred, times, zero_val) for pred in pred_data])\n",
    "        \n",
    "    pred_data = pred_data[0] if pred_data.shape[0] == 1 else pred_data\n",
    "    \n",
    "    return times, pred_data\n",
    "\n",
    "\n",
    "def non_param_predictions(y_train, pred_type):\n",
    "    \n",
    "    if not pred_type in ['sf', 'chf']:\n",
    "        raise ValueError('Models can produce either a survival function (sf) or a cumulative hazard function (chf)')\n",
    "        \n",
    "    train_events = np.array([e for e, _ in y_train])\n",
    "    train_times = np.array([t for _, t in y_train])\n",
    "    \n",
    "    if pred_type == 'sf':\n",
    "        pred_times, pred_data = kaplan_meier_estimator(train_events, train_times)\n",
    "    else:\n",
    "        pred_times, pred_data = nelson_aalen_estimator(train_events, train_times)\n",
    "        \n",
    "    return pred_times, pred_data\n",
    "\n",
    "\n",
    "def model_predictions(X_test, pred_type, model, model_type):\n",
    "    \n",
    "    if not model_type in ['cox', 'rsf']:\n",
    "        raise NotImplementedError('Cox and RSF models are only implemented')\n",
    "    \n",
    "    if not pred_type in ['sf', 'chf']:\n",
    "        raise ValueError('Models can produce either a survival function (sf) or a cumulative hazard function (chf)')\n",
    "        \n",
    "    if pred_type == 'sf':\n",
    "        predictions = model.predict_survival_function(X_test)\n",
    "    else:\n",
    "        predictions = model.predict_cumulative_hazard_function(X_test)\n",
    "        \n",
    "    if model_type == 'cox':\n",
    "        pred_times = predictions[0].x\n",
    "        pred_data = np.array([prediction.a * prediction.y for prediction in predictions])\n",
    "    else:\n",
    "        pred_times = model.event_times_\n",
    "        pred_data = predictions.copy()\n",
    "        \n",
    "    return pred_times, pred_data\n",
    "\n",
    "\n",
    "def mtime_computation(X_test, y_train, model, model_type, t_gamma):\n",
    "    \n",
    "    pred_type = 'sf'\n",
    "    zero_val = 1\n",
    "    \n",
    "    model_times, model_surv = model_predictions(X_test, pred_type, model, model_type)\n",
    "    times, model_surv = time_unification(model_times, model_surv, pred_type, y_train, zero_val)\n",
    "    \n",
    "    measures = np.array(list(times[1:] - times[:-1]) + [t_gamma])\n",
    "    mtimes = np.dot(model_surv, measures)\n",
    "    \n",
    "    return mtimes\n",
    "\n",
    "\n",
    "def limiter(particles, instance, r_clo, hcube):\n",
    "    \n",
    "    r_part = np.linalg.norm(particles - instance, axis=1) \n",
    "    r_lim = np.where(r_part > r_clo, r_clo, r_part)\n",
    "\n",
    "    particles = instance + np.array([coef * point for coef, point in zip(r_lim / r_part, particles - instance)])\n",
    "    \n",
    "    lim_res = []\n",
    "    for component, (c_min, c_max) in zip(particles.T, hcube.T):\n",
    "        component = np.where(component < c_min, c_min, component)\n",
    "        component = np.where(component > c_max, c_max, component)\n",
    "        lim_res.append(component)\n",
    "\n",
    "    return np.transpose(lim_res)\n",
    "\n",
    "\n",
    "def ver_solution(instance, theta, margin, y_train, model, model_type, z_clo, hcube, t_gamma, n_rpoints=1000, n_batch=1000):\n",
    "    \n",
    "    inst_mtime = mtime_computation(instance.reshape(1, -1), y_train, model, model_type, t_gamma)\n",
    "    \n",
    "    if model_type == 'cox':\n",
    "        \n",
    "        print(f'Computing verification solution: {model_type}')\n",
    "        \n",
    "        model_beta = model.coef_\n",
    "        model_times = model.baseline_survival_.x\n",
    "        model_base_surv = model.baseline_survival_.y * model.baseline_survival_.a\n",
    "\n",
    "        model_measures = np.array(list(model_times[1:] - model_times[:-1]) + [t_gamma])\n",
    "        model_mtime = lambda x: model_times[0] + np.dot(model_measures, model_base_surv ** np.exp(x))\n",
    "\n",
    "        u_zero = dichotomy(lambda u: margin - theta * (model_mtime(u) - inst_mtime))\n",
    "\n",
    "        z = cp.Variable(instance.size)\n",
    "\n",
    "        obj = cp.norm(z - instance)\n",
    "        cons = [\n",
    "            np.eye(instance.size) @ z >= hcube[0],\n",
    "            np.eye(instance.size) @ z <= hcube[1],\n",
    "            theta * (model_beta @ z - u_zero) <= 0\n",
    "        ] \n",
    "\n",
    "        prob = cp.Problem(cp.Minimize(obj), cons)\n",
    "        prob.solve()\n",
    "        \n",
    "        z_ver = z.value\n",
    "\n",
    "        return z_ver\n",
    "        \n",
    "    elif model_type == 'rsf':\n",
    "        \n",
    "        print(f'Computing verification solution: {model_type}')\n",
    "        \n",
    "        r_clo = np.linalg.norm(z_clo - instance)\n",
    "        \n",
    "        rpoints = uniform_within_ball(instance, r_clo, n_rpoints)\n",
    "        rpoints = limiter(rpoints, instance, r_clo, hcube)\n",
    "        \n",
    "        indexes, mask_rp = np.array(np.arange(0, n_rpoints, n_batch).tolist() + [n_rpoints]), []\n",
    "\n",
    "        for i in np.arange(1, indexes.size):\n",
    "            \n",
    "            start, end = indexes[i - 1], indexes[i]\n",
    "            print(f'\\tbatch: {start}:{end}')\n",
    "            \n",
    "            rp_mtimes = mtime_computation(rpoints[start:end], y_train, model, model_type, t_gamma)\n",
    "            mask_rp.extend(margin - theta * (rp_mtimes - inst_mtime) <= 0)\n",
    "            \n",
    "        mask_rp = np.array(mask_rp)\n",
    "        \n",
    "        if np.any(mask_rp):\n",
    "            z_ver = rpoints[mask_rp][np.argmin(np.linalg.norm(rpoints[mask_rp] - instance, axis=1))]\n",
    "        else:\n",
    "            z_ver = z_clo\n",
    "        \n",
    "        return z_ver\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        raise NotImplementedError('Cox and RSF models are only implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(z, z_mtime, instance, inst_mtime, theta, margin, mcoef=1e+6):\n",
    "    \n",
    "    loss_dist = np.linalg.norm(z - instance)\n",
    "    loss_marg = margin - theta * (z_mtime - inst_mtime)\n",
    "    \n",
    "    mcoef = 0 if loss_marg < 0 else mcoef\n",
    "    \n",
    "    return loss_dist + mcoef * loss_marg\n",
    "\n",
    "\n",
    "def pso_optimization(obj_func, mtime_func,\n",
    "                     instance, z_clo, r_clo, hcube,\n",
    "                     max_iter=1000, n_particles=2000,\n",
    "                     w=0.729, c1=1.4945, c2=1.4945,\n",
    "                     verbose=20):\n",
    "    \n",
    "    obj_vals = lambda particles: np.array([obj_func(p, m) for p, m in zip(particles, mtime_func(particles))])\n",
    "    \n",
    "    for iter_idx in np.arange(max_iter + 1):\n",
    "    \n",
    "        if iter_idx == 0:\n",
    "            velocities = np.zeros((n_particles, instance.size)) \n",
    "            particles = np.vstack([z_clo, uniform_within_ball(instance, r_clo, n_particles - 1)])\n",
    "            particles = limiter(particles, instance, r_clo, hcube)\n",
    "        else:\n",
    "            terms_cognitive = np.array([np.random.rand() * c1 * p for p in (best_particles - particles)])\n",
    "            terms_social = np.array([np.random.rand() * c2 * p for p in (best_global - particles)])\n",
    "\n",
    "            velocities = w * velocities + terms_cognitive + terms_social\n",
    "            particles = particles + velocities\n",
    "            particles = limiter(particles, instance, r_clo, hcube)\n",
    "\n",
    "        values = obj_vals(particles)\n",
    "\n",
    "        if iter_idx == 0:\n",
    "            best_particles = particles.copy()\n",
    "            best_values = values.copy()\n",
    "        else:\n",
    "            for idx in np.arange(n_particles):\n",
    "                if values[idx] < best_values[idx]:\n",
    "                    best_particles[idx] = particles[idx]\n",
    "                    best_values[idx] = values[idx]\n",
    "\n",
    "        best_value = np.min(best_values)\n",
    "        best_global = best_particles[np.argmin(best_values)]\n",
    "        best_margin = theta * (mtime_func(best_global.reshape(1, -1)) - inst_mtime)\n",
    "\n",
    "        if verbose and (iter_idx % verbose == 0):\n",
    "            print('iter: {:04} | obj: {:.6e} | r: {:2.4f} | z_opt: {}'.format(iter_idx, best_value, best_margin, best_global))\n",
    "\n",
    "    return best_margin, best_value, best_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(info_dom, info_model, info_task, save_path):\n",
    "    \n",
    "    X_train, y_train, hcube = info_dom\n",
    "    model, model_type, t_gamma = info_model\n",
    "    instance, z_clo, z_ver, z_opt, mtime_min, mtime_max = info_task\n",
    "    \n",
    "    #####################################################\n",
    "    \n",
    "    U_train = X_train\n",
    "    U_edges = np.array(list(it.product(*hcube.T)))\n",
    "    u_inst, u_clo, u_ver, u_opt = instance, z_clo, z_ver, z_opt\n",
    "    \n",
    "    #####################################################\n",
    "\n",
    "    U_grid = []\n",
    "    for a, b in hcube.T:\n",
    "        U_grid.append(np.linspace(a, b, 200))\n",
    "    U_grid = np.array(list(it.product(*U_grid)))\n",
    "\n",
    "    grid_mtimes = mtime_computation(U_grid, y_train, model, model_type, t_gamma)\n",
    "    mask_grid = margin - theta * (grid_mtimes - inst_mtime) <= 0\n",
    "    \n",
    "    #####################################################\n",
    "\n",
    "    afunc = lambda p: np.arccos(p[0] / np.linalg.norm(p)) if p[1] >= 0 else 2 * np.pi - np.arccos(p[0] / np.linalg.norm(p))    \n",
    "\n",
    "    U_edges = U_edges[np.argsort([afunc(p) for p in (U_edges - np.mean(U_edges, axis=0))])]\n",
    "    U_edges = np.array(U_edges.tolist() + [U_edges[0]])\n",
    "\n",
    "    u_radius = np.linalg.norm(u_inst - u_clo)\n",
    "    angles = np.linspace(0, 2 * np.pi, 101)\n",
    "    border = u_inst + u_radius * np.array([[np.sin(angle), np.cos(angle)] for angle in angles])\n",
    "    \n",
    "    #####################################################\n",
    "    \n",
    "    mtime_min = np.min([mtime_min, np.min(grid_mtimes)])\n",
    "    mtime_max = np.max([mtime_max, np.max(grid_mtimes)])\n",
    "\n",
    "    cnorm = Normalize(mtime_min, mtime_max)\n",
    "    \n",
    "    #####################################################\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "    plt.title(r'Values of $m(*)$')\n",
    "    plt.scatter(U_grid[:, 0], U_grid[:, 1], s=60,\n",
    "                c=grid_mtimes, cmap='jet', norm=cnorm)\n",
    "    plt.scatter(U_train[:, 0], U_train[:, 1], s=60, edgecolor='black',\n",
    "                c=train_mtimes, cmap='jet', norm=cnorm)\n",
    "    plt.plot(U_edges[:, 0], U_edges[:, 1], c='black', ls='--', label=r'$\\partial \\scrX$')\n",
    "    plt.legend(loc='upper left', framealpha=1)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(r'$x_{1}$')\n",
    "    plt.ylabel(r'$x_{2}$')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.title(r'$m(\\mathbf{x}) = $' + f'{np.round(inst_mtime, 2)}' + r' | $\\theta = $' + f'{theta}' + r' | $r = $' + f'{np.round(margin, 2)}')\n",
    "    plt.scatter(U_grid[mask_grid, 0], U_grid[mask_grid, 1], s=60,\n",
    "                c=grid_mtimes[mask_grid], cmap='jet', norm=cnorm)\n",
    "    plt.scatter(U_train[mask_train, 0], U_train[mask_train, 1], s=60, edgecolor='black',\n",
    "                c=train_mtimes[mask_train], cmap='jet', norm=cnorm)\n",
    "    plt.colorbar()\n",
    "    plt.scatter(u_inst[0], u_inst[1], s=90, c='black', label=r'$\\mathbf{x}$')\n",
    "    plt.plot(U_edges[:, 0], U_edges[:, 1], c='black', ls='--', label=r'$\\partial \\scrX$')\n",
    "    plt.plot(border[:, 0], border[:, 1], c='black', ls='-.', label=r'$\\partial \\scrB$')\n",
    "    plt.scatter(u_opt[0], u_opt[1], s=90, marker='s', edgecolor='black',\n",
    "                c='white', label=r'$\\mathbf{z}_{opt}$')\n",
    "    plt.scatter(u_ver[0], u_ver[1], s=90, marker='^', edgecolor='black',\n",
    "                c='white', label=r'$\\mathbf{z}_{ver}$')\n",
    "    plt.xlabel(r'$x_{1}$')\n",
    "    plt.ylabel(r'$x_{2}$')\n",
    "    plt.legend(loc='upper left', framealpha=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = False\n",
    "\n",
    "if flag:\n",
    "    n_points, l, k, max_time = 1000, 1e-5, 2, 100000\n",
    "\n",
    "    n_features = 2\n",
    "    beta_true, X, y = data_gen(n_points, n_features, l, k, max_time)\n",
    "\n",
    "    with open('data/sdata_d{:02}.pkl'.format(n_features), 'wb') as f:\n",
    "        pickle.dump([beta_true, X, y], f)\n",
    "\n",
    "    n_features = 20\n",
    "    beta_true, X, y = data_gen(n_points, n_features, l, k, max_time)\n",
    "\n",
    "    with open('data/sdata_d{:02}.pkl'.format(n_features), 'wb') as f:\n",
    "        pickle.dump([beta_true, X, y], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "names = ['sdata_d02', 'sdata_d20', 'stanford2', 'myeloid_(trt_A)', 'myeloid_(trt_B)']\n",
    "\n",
    "###### Synthetic data: dim = 2\n",
    "\n",
    "with open('data/sdata_d02.pkl', 'rb') as f:\n",
    "    bt, X, y = pickle.load(f)\n",
    "    \n",
    "data.append([X, y])\n",
    "\n",
    "###### Synthetic data: dim = 20\n",
    "\n",
    "with open('data/sdata_d20.pkl', 'rb') as f:\n",
    "    bt, X, y = pickle.load(f)\n",
    "    \n",
    "data.append([X, y])\n",
    "\n",
    "###### Real data: stanford2\n",
    "\n",
    "dtmp = pd.read_table('data/stanford2.csv', sep=';')\n",
    "X = dtmp.values[:, :-2]\n",
    "y = np.array([(e, t) for t, e in dtmp.values[:, -2:]], dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "\n",
    "data.append([X, y])\n",
    "\n",
    "###### Real data: myeloid (trt_A & trt_B)\n",
    "\n",
    "dtmp = pd.read_table('data/myeloid.csv', sep=';')\n",
    "\n",
    "for trt_B in [0, 1]:\n",
    "    dtmp_cut = dtmp[dtmp['trt_B'] == trt_B]\n",
    "    dtmp_cut = dtmp_cut.drop(columns=['trt_B'])\n",
    "\n",
    "    X = dtmp_cut.values[:, :-2]\n",
    "    y = np.array([(e, t) for t, e in dtmp_cut.values[:, -2:]], dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "    \n",
    "    data.append([X, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_type = 'cox'\n",
    "model_type = 'rsf'\n",
    "\n",
    "if model_type == 'cox':\n",
    "    model = CoxPHSurvivalAnalysis()\n",
    "else:\n",
    "    model = RandomSurvivalForest(n_estimators=250, min_samples_leaf=20, n_jobs=-1, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_type = 'sf'\n",
    "zero_val = 1\n",
    "t_gamma = 1\n",
    "\n",
    "n_test = 2\n",
    "n_rpoints = 10 ** 6\n",
    "n_batch = 10 ** 5\n",
    "\n",
    "max_iter = 1000\n",
    "verbose = 50\n",
    "\n",
    "for (X_train, y_train), name in zip(data, names):\n",
    "    \n",
    "    print(f'\\nDataset: {name}\\n')\n",
    "    \n",
    "    hcube = np.array([np.min(X_train, axis=0), np.max(X_train, axis=0)])\n",
    "    X_test = np.random.RandomState(seed=1234).uniform(low=hcube[0], high=hcube[1], size=(n_test, hcube.shape[1]))\n",
    "    X_test = np.array([[instance] * 2 for instance in X_test]).reshape(2 * n_test, -1)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    for task_idx, (theta, instance) in enumerate(list(zip([1, -1, 1, -1], X_test)), start=1):\n",
    "        \n",
    "        print(f'task: {task_idx}\\n')\n",
    "        \n",
    "        margin = None\n",
    "        \n",
    "        model_times, model_surv = model_predictions(np.vstack([X_train, instance]), pred_type, model, model_type)\n",
    "        times, model_surv = time_unification(model_times, model_surv, pred_type, y_train, zero_val)\n",
    "\n",
    "        train_surv, inst_surv = model_surv[:-1], model_surv[-1]\n",
    "        measures = np.array(list(times[1:] - times[:-1]) + [t_gamma])\n",
    "\n",
    "        train_mtimes = np.dot(train_surv, measures)\n",
    "        inst_mtime = np.dot(inst_surv, measures)\n",
    "        \n",
    "        mtime_min = np.min(train_mtimes)\n",
    "        mtime_max = np.max(train_mtimes)\n",
    "\n",
    "        if not (mtime_min <= inst_mtime <= mtime_max):\n",
    "            mtime_min = np.min([t for _, t in y_train])\n",
    "            mtime_max = np.max([t for _, t in y_train])\n",
    "\n",
    "        margin_max = 0.5 * ((1 - theta) * (inst_mtime - mtime_min) + (1 + theta) * (mtime_max - inst_mtime)) \n",
    "\n",
    "        if not (margin is None):\n",
    "            if not (0 < margin < margin_max):\n",
    "                raise ValueError(f'margin is out of interval ({0.}, {margin_max})')\n",
    "        else:\n",
    "            margin = np.random.uniform(low=0.25, high=0.75) * margin_max\n",
    "\n",
    "        mask_train = margin - theta * (train_mtimes - inst_mtime) <= 0\n",
    "        \n",
    "        if np.any(mask_train):\n",
    "            idx_clo = np.argmin(np.linalg.norm(X_train[mask_train] - instance, axis=1))\n",
    "            z_clo = X_train[mask_train][idx_clo]\n",
    "        else:\n",
    "            print('There is no the closest train point\\n')\n",
    "            edges = np.array(list(it.product(*hcube.T)))\n",
    "            idx_clo = np.argmax(np.linalg.norm(edges - instance, axis=1))\n",
    "            z_clo = edges[idx_clo]    \n",
    "        r_clo = np.linalg.norm(instance - z_clo)\n",
    "            \n",
    "        z_ver = ver_solution(instance, theta, margin, y_train, model, model_type, z_clo, hcube, t_gamma, n_rpoints, n_batch)\n",
    "        z_ver_mtime = mtime_computation(z_ver.reshape(1, -1), y_train, model, model_type, t_gamma)\n",
    "        z_ver_margin = theta * (z_ver_mtime - inst_mtime)\n",
    "        z_ver_dist = np.linalg.norm(z_ver - instance)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        obj_func = lambda particle, part_mtime: objective(particle, part_mtime, instance, inst_mtime, theta, margin)\n",
    "        mtime_func = lambda particles: mtime_computation(particles, y_train, model, model_type, t_gamma)\n",
    "\n",
    "        z_opt_margin, z_opt_dist, z_opt = pso_optimization(obj_func, mtime_func, instance, z_clo, r_clo, hcube,\n",
    "                                                           max_iter=max_iter, verbose=verbose)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        with open(f'results/experiment/{name}_{model_type}_task_{task_idx}.pkl', 'wb') as f:\n",
    "            task = [instance, inst_mtime, theta, margin]\n",
    "            info_dom = [hcube, train_mtimes, mtime_min, mtime_max, margin_max, mask_train, z_clo]\n",
    "            info_ver = [z_ver, z_ver_mtime, z_ver_margin, z_ver_dist]\n",
    "            info_opt = [z_opt, z_opt_margin, z_opt_dist]\n",
    "            pickle.dump([task, info_dom, info_ver, info_opt], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_type = 'rsf'\n",
    "\n",
    "if model_type == 'cox':\n",
    "    model = CoxPHSurvivalAnalysis()\n",
    "else:\n",
    "    model = RandomSurvivalForest(n_estimators=250, min_samples_leaf=20, n_jobs=-1, random_state=1234)\n",
    "\n",
    "tab = []\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    if name == 'sdata_d02':\n",
    "        \n",
    "        with open('data/sdata_d02.pkl', 'rb') as f:\n",
    "            _, X_train, y_train = pickle.load(f)\n",
    "            \n",
    "    elif name == 'sdata_d20':\n",
    "\n",
    "        with open('data/sdata_d20.pkl', 'rb') as f:\n",
    "            _, X_train, y_train = pickle.load(f)\n",
    "    \n",
    "    elif name == 'stanford2':\n",
    "\n",
    "        dtmp = pd.read_table('data/stanford2.csv', sep=';')\n",
    "        X_train = dtmp.values[:, :-2]\n",
    "        y_train = np.array([(e, t) for t, e in dtmp.values[:, -2:]], dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "\n",
    "    elif name == 'myeloid_(trt_A)':\n",
    "\n",
    "        dtmp = pd.read_table('data/myeloid.csv', sep=';')\n",
    "\n",
    "        trt_B = 0\n",
    "        dtmp_cut = dtmp[dtmp['trt_B'] == trt_B]\n",
    "        dtmp_cut = dtmp_cut.drop(columns=['trt_B'])\n",
    "\n",
    "        X_train = dtmp_cut.values[:, :-2]\n",
    "        y_train = np.array([(e, t) for t, e in dtmp_cut.values[:, -2:]], dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        dtmp = pd.read_table('data/myeloid.csv', sep=';')\n",
    "\n",
    "        trt_B = 1\n",
    "        dtmp_cut = dtmp[dtmp['trt_B'] == trt_B]\n",
    "        dtmp_cut = dtmp_cut.drop(columns=['trt_B'])\n",
    "\n",
    "        X_train = dtmp_cut.values[:, :-2]\n",
    "        y_train = np.array([(e, t) for t, e in dtmp_cut.values[:, -2:]], dtype=[('Status', '?'), ('Survival_in_days', '<f8')])\n",
    "        \n",
    "    model.fit(X_train, y_train)   \n",
    "    \n",
    "    paths = sorted(glob.glob(f'results/experiment/{name}_{model_type}*'))\n",
    "    \n",
    "    for path in paths:\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            task, info_dom, info_ver, info_opt = pickle.load(f)\n",
    "            \n",
    "        instance, inst_mtime, theta, margin = task\n",
    "        hcube, train_mtimes, mtime_min, mtime_max, margin_max, mask_train, z_clo = info_dom\n",
    "        z_ver, _, z_ver_margin, z_ver_dist = info_ver\n",
    "        z_opt, z_opt_margin, z_opt_dist = info_opt\n",
    "\n",
    "        tab.append([theta, margin, z_ver_margin, z_opt_margin, z_ver_dist, z_opt_dist, np.linalg.norm(z_ver - z_opt)])\n",
    "        \n",
    "        if X_train.shape[1] == 2:\n",
    "            info_dom = [X_train, y_train, hcube]\n",
    "            info_model = [model, model_type, t_gamma]\n",
    "            info_task = [instance, z_clo, z_ver, z_opt, mtime_min, mtime_max]\n",
    "            save_path = 'results/' + path.split('\\\\')[-1][:-4] + '.png'\n",
    "\n",
    "            plot_result(info_dom, info_model, info_task, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_columns = np.array(['theta', 'task_margin', 'ver_margin', 'sol_margin', 'ver_dist', 'sol_dist', 'dist(ver, sol)'])\n",
    "tab_index = np.array([[name] * 4 for name in names]).flatten()\n",
    "\n",
    "tab_res = pd.DataFrame(tab, columns=tab_columns, index=tab_index)\n",
    "tab_res.to_csv(f'results/results_{model_type}.tsv', sep='\\t')\n",
    "tab_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
